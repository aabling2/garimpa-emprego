{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to install google packages**\n",
    "\n",
    "pip install google\n",
    "\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**search(query, tld='com', lang='en', num=10, start=0, stop=None, pause=2.0)**\n",
    "\n",
    "query : query string that we want to search for.\n",
    "\n",
    "tld : tld stands for top level domain which means we want to search our result on google.com or google.in or some other domain.\n",
    "\n",
    "lang : lang stands for language.\n",
    "\n",
    "num : Number of results we want.\n",
    "\n",
    "start : First result to retrieve.\n",
    "\n",
    "stop : Last result to retrieve. Use None to keep searching forever.\n",
    "\n",
    "pause : Lapse to wait between HTTP requests. Lapse too short may cause Google to block your IP. Keeping significant lapse will make your program slow but its safe and better option.\n",
    "\n",
    "Return : Generator (iterator) that yields found URLs. If the stop parameter is None the iterator will loop forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import os\n",
    "import os.path\n",
    "import requests\n",
    "import re\n",
    "import webbrowser\n",
    "from googlesearch import search \n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funções OK\n"
     ]
    }
   ],
   "source": [
    "# Function to create folders\n",
    "def Create_folder(folder_name):\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(folder_name) \n",
    "        print('Pasta \"', folder_name, '\" criada com sucesso!')\n",
    "    \n",
    "    except FileExistsError:\n",
    "        print('Pasta \"', folder_name, '\" já existe!')\n",
    "\n",
    "\n",
    "# Function to read saved files with url links and pass to a bag of words\n",
    "def Load_bag_links(keys):\n",
    "\n",
    "    file_exist = False\n",
    "    file_name = 'Links/' + keys + '.txt'\n",
    "    \n",
    "    if os.path.isfile(file_name) == True:\n",
    "        register_links = open(file_name,'r')\n",
    "        file_exist = True\n",
    "        txt_links = register_links.read()\n",
    "        register_links.close()\n",
    "        \n",
    "        bag_converted = eval(txt_links)\n",
    "        bag_links.update(bag_converted)\n",
    "        \n",
    "        print(\"\\nArquivo de links: \", file_name)     \n",
    "        \n",
    "    else:\n",
    "        file_exist = False\n",
    "        print(\"\\nArquivo não existe: \", file_name)   \n",
    "    \n",
    "    return bag_links\n",
    "\n",
    "# Function to load the backup of results\n",
    "def Load_bag_results():\n",
    "    \n",
    "    file_name = 'Backups/' + (os.path.basename(os.getcwd())) + '-results-backup.txt'\n",
    "    \n",
    "    try:\n",
    "        register_links_result = open(file_name,'r')\n",
    "        txt_links = register_links_result.read()\n",
    "        bag_converted = eval(txt_links)\n",
    "        bag_links_result.update(bag_converted)\n",
    "        register_links_result.close()\n",
    "    except:\n",
    "        register_links_result = open(file_name,'w')\n",
    "        register_links_result.close()\n",
    "        \n",
    "    return bag_links_result\n",
    "\n",
    "# Function to request content of the url\n",
    "def Request_content(url):\n",
    "    try:\n",
    "        request = requests.get(url, allow_redirects=True, auth=('user', 'pass'), timeout=10)            \n",
    "        if request.status_code == 200:\n",
    "            print('A página é válida')\n",
    "            soup = BeautifulSoup(request.content, 'lxml')\n",
    "            r_bool = True\n",
    "        else:\n",
    "            print('A página NÃO é válida')\n",
    "            r_bool = False\n",
    "    except:\n",
    "        print('A página NÃO é válida')\n",
    "        r_bool = False\n",
    "        \n",
    "    return soup, r_bool\n",
    "\n",
    "print('Funções OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Visao Computacional': 5.0, 'Inteligencia Artificial': 3.0, 'GPU': 3.0, 'CUDA': 3.0, 'Qt Creator': 3.0, 'OpenCV': 3.0, 'Unreal Engine': 3.0, 'Unity': 3.0, 'Projetos': 3.0, 'C++': 3.0, 'Python': 3.0, 'Programador': 3.0, 'Engenharia Eletrica': 3.0, 'Mecatronica': 3.0, 'Automação': 3.0, 'Robotica': 3.0, 'Projetista': 2.5, 'Desenvolvedor': 2.5, 'Desenhista': 2.5, 'Cadista': 2.5, 'Cad': 2.5, 'Autocad': 2.5}\n",
      "{'Vaga': 2.0, 'Emprego': 2.0, 'Oportunidade': 2.0, 'Startup': 2.0, 'Junior': 2.0, 'Pleno': 2.0, 'Auxiliar': 2.0, 'Tecnico': 2.0, 'Assistente': 2.0, 'Analista': 2.0, 'Trainee': 2.0, 'Engenheiro': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Bag of links\n",
    "bag_links = {}\n",
    "\n",
    "# Keywords to search\n",
    "keys_lv = {}\n",
    "\n",
    "# Level 1 highest priority to search loops / weights / don't put too much key, provide error of requests\n",
    "lv1 = 5.0\n",
    "lv2 = 3.0\n",
    "lv3 = 2.5\n",
    "lv4 = 2.0\n",
    "\n",
    "# Dictionary of keywords / use uppercase at first character / maximum 2 words\n",
    "keys_lv1 = {}.fromkeys(['Visao Computacional'],lv1)\n",
    "keys_lv2 = {}.fromkeys(['Inteligencia Artificial', 'GPU', 'CUDA', 'Qt Creator', 'OpenCV', 'Unreal Engine', 'Unity', 'Projetos', \n",
    "                        'C++', 'Python', 'Programador', 'Engenharia Eletrica', 'Mecatronica', 'Automação', 'Robotica'],lv2)\n",
    "keys_lv3 = {}.fromkeys(['Projetista', 'Desenvolvedor', 'Desenhista', 'Cadista', 'Cad', 'Autocad'],lv3)\n",
    "\n",
    "# Dictionary of auxiliary keywords to find jobs / join the main key\n",
    "keys_init = {}.fromkeys(['Vaga', 'Emprego', 'Oportunidade', 'Startup', 'Junior', 'Pleno', 'Auxiliar', 'Tecnico', 'Assistente', 'Analista', 'Trainee', 'Engenheiro'],lv4)\n",
    "\n",
    "keys_lv.update(keys_lv1)\n",
    "keys_lv.update(keys_lv2)\n",
    "keys_lv.update(keys_lv3)\n",
    "\n",
    "print(keys_lv)\n",
    "print(keys_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta \" Links \" já existe!\n",
      "\u001b[1mVaga Visao Computacional\u001b[0;0m\n",
      "\u001b[1mEmprego Visao Computacional\u001b[0;0m\n",
      "\u001b[1mOportunidade Visao Computacional\u001b[0;0m\n",
      "\u001b[1mStartup Visao Computacional\u001b[0;0m\n",
      "\u001b[1mJunior Visao Computacional\u001b[0;0m\n"
     ]
    }
   ],
   "source": [
    "# Code to research and save links\n",
    "\n",
    "# Number of searches for each subject\n",
    "num_search = 100\n",
    "\n",
    "Create_folder('Links')\n",
    "\n",
    "# Set keys_lv for all keywords or just one level\n",
    "for keys in keys_lv1:\n",
    "    try:\n",
    "        bag_links = {}\n",
    "        file_exist = False\n",
    "        time_delay = len(keys_lv)+len(keys_init)\n",
    "        file_name = 'Links/' + keys + '.txt'\n",
    "\n",
    "        if os.path.isfile(file_name) == True:\n",
    "            file_exist = True\n",
    "        else:\n",
    "            file_exist = False\n",
    "            register_links = open(file_name,'w')\n",
    "\n",
    "        for init in keys_init:    \n",
    "            if file_exist == True:\n",
    "                print('Arquivo de links já existe: ' + file_name)\n",
    "                register_links.close()   \n",
    "                break\n",
    "            \n",
    "            print(\"\\033[1m\" + init + ' ' + keys + \"\\033[0;0m\")\n",
    "            query = init + ' ' + keys\n",
    "            \n",
    "            # Pause with long time delay will mantain the connection and not block when have lot of queries \n",
    "            for link_finded in search(query, tld=\"co.in\", lang='pt', num=num_search, stop=num_search, pause=time_delay*1.5):    \n",
    "                bag_links[link_finded] = 0\n",
    "\n",
    "        if file_exist == False:\n",
    "            txt_data = str(bag_links)\n",
    "            register_links.write(txt_data)\n",
    "            register_links.close()\n",
    "            print(\"Arquivo salvo: \", file_name)\n",
    "            \n",
    "    except:\n",
    "        if file_exist == False:\n",
    "            register_links.close()\n",
    "            os.remove(file_name)\n",
    "            print(\"Erro na requisição de páginas\")\n",
    "            print(\"Arquivo removido: \", file_name)\n",
    "            break\n",
    "        \n",
    "print(\"Pesquisa finalizada!\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code of webscrapring the links and apply weights\n",
    "bag_links_result = {}\n",
    "r_bool = True\n",
    "keys_lv_init = {}\n",
    "keys_lv_init.update(keys_lv)\n",
    "keys_lv_init.update(keys_init)\n",
    "progress = 0\n",
    "\n",
    "# Special condition when search double '+' \n",
    "if 'C++' in keys_lv_init:\n",
    "    keys_lv_init['C+'] = keys_lv_init.pop('C++')\n",
    "    \n",
    "Create_folder('Backups')\n",
    "    \n",
    "# Read data from txt to load links already processed to bag_links_result\n",
    "bag_links_results = Load_bag_results()\n",
    "\n",
    "# Set keys_lv for all keywords or just one level\n",
    "for keys in keys_lv1:\n",
    "\n",
    "    bag_links = {}\n",
    "    bag_links = Load_bag_links(keys)\n",
    "    \n",
    "    # Delete links in bag_links that already exists on results\n",
    "    for i in bag_links_result:\n",
    "        if i in bag_links:\n",
    "            del bag_links[i]\n",
    "    \n",
    "    bag_links_index = []\n",
    "    for index_link, link in enumerate(bag_links):\n",
    "        bag_links_index.append(link)\n",
    "    \n",
    "    for url in bag_links:\n",
    "        \n",
    "        if url.split('.')[-1] == 'pdf':\n",
    "            continue\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        file_name = 'Links/' + keys + '.txt'\n",
    "        print(\"\\nArquivo de links: \", file_name, ' -- Qtd. links: ', len(bag_links))   \n",
    "        print('--> %.2f'%progress, '%')\n",
    "        print('\\n\\033[1m'+url+'\\033[0;0m')\n",
    "\n",
    "        soup, r_bool = Request_content(url)\n",
    "        \n",
    "        for searched_word in keys_lv_init:\n",
    "\n",
    "            if r_bool is False:\n",
    "                break\n",
    "\n",
    "            keys_lv_init[searched_word.lower()] = keys_lv_init.value()            \n",
    "\n",
    "            results = soup.find_all(string=re.compile(searched_word))\n",
    "\n",
    "            words = None\n",
    "            searched_word_split = None\n",
    "            searched_word_split = searched_word.split()\n",
    "\n",
    "            for text in results:\n",
    "                words = text.split()\n",
    "\n",
    "                for index, word in enumerate(words):\n",
    "\n",
    "                    for split_word in searched_word_split:\n",
    "                        if split_word in word:\n",
    "                            word = split_word\n",
    "\n",
    "                    word_double = None\n",
    "                    if index != 0:\n",
    "                        word_double = words[index-1]+' '+word\n",
    "\n",
    "                    if word==searched_word or word_double==searched_word:         \n",
    "                        bag_links[url] += keys_lv_init[searched_word_original]\n",
    "                        print('Palavra encontrada: ', searched_word)\n",
    "\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                break\n",
    "\n",
    "        # Update status of processing\n",
    "        progress = ((1+bag_links_index.index(url))/len(bag_links_index))*100\n",
    "        \n",
    "        # Update results\n",
    "        bag_links_result[url] = bag_links[url]\n",
    "\n",
    "        # Save backup of results each link processed\n",
    "        file_name = 'Backups/' + (os.path.basename(os.getcwd())) + '-results-backup.txt'\n",
    "        register_links_result = open(file_name,'w')\n",
    "        txt_data = str(bag_links_result)\n",
    "        register_links_result.write(txt_data)\n",
    "        register_links_result.close()\n",
    "        print(\"Backup salvo!\")\n",
    "\n",
    "    print('\\nVarredura completa para: ', keys)\n",
    "\n",
    "print('\\nVarredura de páginas completa!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to organize by the weights\n",
    "bag_links_sorted = sorted(bag_links_result.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, bags in bag_links_sorted:\n",
    "    bag_links_rank = [idx for idx, val in bag_links_sorted]\n",
    "\n",
    "print('\\033[1mTotal de links: ', len(bag_links_rank), '\\033[0;0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to limit rank and create a HTML archive with the results\n",
    "limit_rank = 1000\n",
    "html_links = \"\"\n",
    "html_keys1 = \"\"\n",
    "html_keys2 = \"\"\n",
    "html_keys3 = \"\"\n",
    "\n",
    "file_name_html = 'G_Emprego-Resultados.html'\n",
    "f = open(file_name_html,'w')   \n",
    "\n",
    "for search_word in keys_lv1.keys():\n",
    "    html_keys1 += ' ' + search_word + ' /'\n",
    "\n",
    "for search_word in keys_lv2.keys():\n",
    "    html_keys2 += ' ' + search_word + ' /'\n",
    "    \n",
    "for search_word in keys_lv3.keys():\n",
    "    html_keys3 += ' ' + search_word + ' /'\n",
    "\n",
    "count = 1\n",
    "for url in bag_links_rank:\n",
    "    html_links += '<p>' + str(bag_links_rank.index(url)+1) + '- <a href=' + url + '>' + url + '</a></p>'\n",
    "    count+=1\n",
    "    if count > limit_rank:\n",
    "        break\n",
    "        \n",
    "html_code = '<html>' + '<p>Prioridade 1 -->' + html_keys1 + '</p><p>Prioridade 2 -->' + html_keys2 + '</p><p>Prioridade 3 -->' + html_keys3 + '</p>' + html_links + '</html>'\n",
    "\n",
    "f.write(html_code)\n",
    "\n",
    "webbrowser.open(file_name_html)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to list keywords with best match researches / just if used all keywords to research links\n",
    "keys_compare = {}\n",
    "bag_links = {}\n",
    "keys_compare.update(keys_lv)\n",
    "\n",
    "for key in keys_lv:\n",
    "    bag_links = Load_bag_links(key)\n",
    "    print(\"Arquivo: \", key, \"Links: \", len(bag_links))\n",
    "    for url in bag_links:\n",
    "        if url in bag_links_rank:\n",
    "            keys_compare[key] += 1\n",
    "            \n",
    "keys_sorted = sorted(keys_compare.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print('\\n', keys_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
