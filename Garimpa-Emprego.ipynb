{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import os\n",
    "import os.path\n",
    "import requests\n",
    "import re\n",
    "import webbrowser\n",
    "from googlesearch import search \n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create folders\n",
    "def Create_folder(folder_name):\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(folder_name) \n",
    "        print('Pasta \"', folder_name, '\" criada com sucesso!')\n",
    "    \n",
    "    except FileExistsError:\n",
    "        print('Pasta \"', folder_name, '\" já existe!')\n",
    "\n",
    "\n",
    "# Function to read saved files with url links and pass to a bag of words\n",
    "def Load_bag_links(keys):\n",
    "\n",
    "    file_exist = False\n",
    "    file_name = 'Links/' + keys + '.txt'\n",
    "    \n",
    "    if os.path.isfile(file_name) == True:\n",
    "        register_links = open(file_name,'r')\n",
    "        file_exist = True\n",
    "        txt_links = register_links.read()\n",
    "        register_links.close()\n",
    "        \n",
    "        bag_converted = eval(txt_links)\n",
    "        bag_links.update(bag_converted)\n",
    "        \n",
    "        print(\"\\nArquivo de links: \", file_name)     \n",
    "        \n",
    "    else:\n",
    "        file_exist = False\n",
    "        print(\"\\nArquivo não existe: \", file_name)   \n",
    "    \n",
    "    return bag_links\n",
    "\n",
    "# Function to load the backup of results\n",
    "def Load_bag_results():\n",
    "    \n",
    "    file_name = 'Backups/' + (os.path.basename(os.getcwd())) + '-results-backup.txt'\n",
    "    \n",
    "    try:\n",
    "        register_links_result = open(file_name,'r')\n",
    "        txt_links = register_links_result.read()\n",
    "        bag_converted = eval(txt_links)\n",
    "        bag_links_result.update(bag_converted)\n",
    "        register_links_result.close()\n",
    "    except:\n",
    "        register_links_result = open(file_name,'w')\n",
    "        register_links_result.close()\n",
    "        \n",
    "    return bag_links_result\n",
    "\n",
    "# Function to request content of the url\n",
    "def Request_content(url):\n",
    "    soup_url = {}\n",
    "    try:\n",
    "        request = requests.get(url, allow_redirects=True, auth=('user', 'pass'), timeout=10)            \n",
    "        if request.status_code == 200:\n",
    "            print('A página é válida')\n",
    "            soup_url = BeautifulSoup(request.content, 'lxml')\n",
    "            r_bool_url = True\n",
    "        else:\n",
    "            print('A página NÃO é válida')\n",
    "            r_bool_url = False\n",
    "    except:\n",
    "        print('A página NÃO é válida')\n",
    "        r_bool_url = False\n",
    "        \n",
    "    return soup_url, r_bool_url\n",
    "\n",
    "print('Funções OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to init inputs of the searching\n",
    "# Bag of links\n",
    "bag_links = {}\n",
    "\n",
    "# Keywords to search\n",
    "keys_lv = {}\n",
    "\n",
    "# Level 1 highest priority to search loops / weights / don't put too much key, provide error of requests\n",
    "lv1 = 5.0\n",
    "lv2 = 3.0\n",
    "lv3 = 2.5\n",
    "lv4 = 2.0\n",
    "\n",
    "# Dictionary of keywords / use uppercase at first character / maximum 2 words\n",
    "keys_lv1 = {}.fromkeys(['Visao Computacional', 'Computer Vision'],lv1)\n",
    "keys_lv2 = {}.fromkeys(['Inteligencia Artificial', 'GPU', 'CUDA', 'OpenCV','Projetos', \n",
    "                        'C++', 'Python', 'Programador', 'Engenharia Eletrica', 'Mecatronica', 'Automação', 'Robotica'],lv2)\n",
    "keys_lv3 = {}.fromkeys(['Projetista', 'Desenvolvedor'],lv3)\n",
    "\n",
    "# Dictionary of auxiliary keywords to find jobs / join the main key\n",
    "keys_init = {}.fromkeys(['Vaga', 'Emprego', 'Oportunidade', 'Startup', 'Junior', 'Pleno', 'Auxiliar', 'Tecnico', 'Assistente', 'Analista', 'Trainee', 'Engenheiro'],lv4)\n",
    "\n",
    "keys_lv.update(keys_lv1)\n",
    "keys_lv.update(keys_lv2)\n",
    "keys_lv.update(keys_lv3)\n",
    "\n",
    "print(keys_lv)\n",
    "print(keys_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to research and save links\n",
    "\n",
    "# Number of searches for each subject\n",
    "num_search = 100\n",
    "\n",
    "# Delay for requests pause\n",
    "time_delay = len(keys_lv)+len(keys_init)\n",
    "\n",
    "Create_folder('Links')\n",
    "\n",
    "# Set keys_lv for all keywords or just one level\n",
    "for keys in keys_lv1:\n",
    "    try:\n",
    "        bag_links = {}\n",
    "        file_exist = False\n",
    "        \n",
    "        file_name = 'Links/' + keys + '.txt'\n",
    "\n",
    "        if os.path.isfile(file_name) == True:\n",
    "            file_exist = True\n",
    "        else:\n",
    "            file_exist = False\n",
    "            register_links = open(file_name,'w')\n",
    "\n",
    "        for init in keys_init:    \n",
    "            if file_exist == True:\n",
    "                print('Arquivo de links já existe: ' + file_name)\n",
    "                register_links.close()   \n",
    "                break\n",
    "            \n",
    "            print(\"\\033[1m\" + init + ' ' + keys + \"\\033[0;0m\")\n",
    "            query = init + ' ' + keys\n",
    "            \n",
    "            # Pause with long time delay will mantain the connection and not block when have lot of queries \n",
    "            for link_finded in search(query, tld=\"com\", lang='pt', num=num_search, stop=num_search, pause=time_delay*3):    \n",
    "                bag_links[link_finded] = 0\n",
    "                \n",
    "        if file_exist == False:\n",
    "            txt_data = str(bag_links)\n",
    "            register_links.write(txt_data)\n",
    "            register_links.close()\n",
    "            print(\"Arquivo salvo: \", file_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        if file_exist == False:\n",
    "            register_links.close()\n",
    "            os.remove(file_name)\n",
    "            print(e)\n",
    "            print(\"Arquivo removido: \", file_name)\n",
    "            break\n",
    "        \n",
    "print(\"Pesquisa finalizada!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code of webscrapring the links and apply weights\n",
    "bag_links_result = {}\n",
    "r_bool = True\n",
    "keys_lv_init = {}\n",
    "keys_lv_init.update(keys_lv)\n",
    "keys_lv_init.update(keys_init)\n",
    "progress = 0\n",
    "\n",
    "# Special condition when search double '+' \n",
    "if 'C++' in keys_lv_init:\n",
    "    keys_lv_init['C+'] = keys_lv_init.pop('C++')\n",
    "    \n",
    "# Include lower case keys\n",
    "lower_keys_lv_init = {}\n",
    "for k, v in keys_lv_init.items():\n",
    "    lower_keys_lv_init[k.lower()] = v\n",
    "keys_lv_init.update(lower_keys_lv_init)\n",
    "    \n",
    "Create_folder('Backups')\n",
    "    \n",
    "# Read data from txt to load links already processed to bag_links_result\n",
    "bag_links_results = Load_bag_results()\n",
    "\n",
    "# Set keys_lv for all keywords or just one level\n",
    "for keys in keys_lv1:\n",
    "\n",
    "    bag_links = {}\n",
    "    bag_links = Load_bag_links(keys)\n",
    "    \n",
    "    # Delete links in bag_links that already exists on results\n",
    "    for i in bag_links_result:\n",
    "        if i in bag_links:\n",
    "            del bag_links[i]\n",
    "    \n",
    "    bag_links_index = []\n",
    "    for index_link, link in enumerate(bag_links):\n",
    "        bag_links_index.append(link)\n",
    "    \n",
    "    for url in bag_links:\n",
    "        \n",
    "        if url.split('.')[-1] == 'pdf':\n",
    "            continue\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        file_name = 'Links/' + keys + '.txt'\n",
    "        print(\"\\nArquivo de links: \", file_name, ' -- Qtd. links: ', len(bag_links))   \n",
    "        print('--> %.2f'%progress, '%')\n",
    "        print('\\n\\033[1m'+url+'\\033[0;0m')\n",
    "\n",
    "        # Request pages and get the soup of it\n",
    "        soup, r_bool = Request_content(url)\n",
    "        \n",
    "        for searched_word in keys_lv_init:\n",
    "\n",
    "            if r_bool is False:\n",
    "                break\n",
    "\n",
    "            results = soup.find_all(string=re.compile(searched_word))\n",
    "            \n",
    "            words = None\n",
    "            searched_word_split = None\n",
    "            searched_word_split = searched_word.split()\n",
    "\n",
    "            for text in results:\n",
    "                words = text.split()\n",
    "\n",
    "                for index, word in enumerate(words):\n",
    "\n",
    "                    for split_word in searched_word_split:\n",
    "                        if split_word in word:\n",
    "                            word = split_word\n",
    "\n",
    "                    word_double = None\n",
    "                    if index != 0:\n",
    "                        word_double = words[index-1]+' '+word\n",
    "\n",
    "                    if word==searched_word or word_double==searched_word:         \n",
    "                        bag_links[url] += keys_lv_init[searched_word]\n",
    "                        print('Palavra encontrada: ', searched_word)\n",
    "\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                break\n",
    "\n",
    "        # Update status of processing\n",
    "        progress = ((1+bag_links_index.index(url))/len(bag_links_index))*100\n",
    "        \n",
    "        # Update results\n",
    "        bag_links_result[url] = bag_links[url]\n",
    "\n",
    "        # Save backup of results each link processed\n",
    "        file_name = 'Backups/' + (os.path.basename(os.getcwd())) + '-results-backup.txt'\n",
    "        register_links_result = open(file_name,'w')\n",
    "        txt_data = str(bag_links_result)\n",
    "        register_links_result.write(txt_data)\n",
    "        register_links_result.close()\n",
    "        print(\"Backup salvo!\")\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print('\\nVarredura completa para: ', keys)\n",
    "\n",
    "print('\\nVarredura de páginas completa!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to organize by the weights\n",
    "bag_links_sorted = sorted(bag_links_result.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, bags in bag_links_sorted:\n",
    "    bag_links_rank = [idx for idx, val in bag_links_sorted]\n",
    "\n",
    "print('\\033[1mTotal de links: ', len(bag_links_rank), '\\033[0;0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to limit rank and create a HTML archive with the results\n",
    "limit_rank = 200\n",
    "html_links = \"\"\n",
    "html_keys1 = \"\"\n",
    "html_keys2 = \"\"\n",
    "html_keys3 = \"\"\n",
    "\n",
    "file_name_html = 'G_Emprego-Resultados.html'\n",
    "f = open(file_name_html,'w')   \n",
    "\n",
    "for search_word in keys_lv1.keys():\n",
    "    html_keys1 += ' ' + search_word + ' /'\n",
    "\n",
    "for search_word in keys_lv2.keys():\n",
    "    html_keys2 += ' ' + search_word + ' /'\n",
    "    \n",
    "for search_word in keys_lv3.keys():\n",
    "    html_keys3 += ' ' + search_word + ' /'\n",
    "\n",
    "count = 1\n",
    "for url in bag_links_rank:\n",
    "    html_links += '<p>' + str(bag_links_rank.index(url)+1) + '- <a href=' + url + '>' + url + '</a></p>'\n",
    "    count+=1\n",
    "    if count > limit_rank:\n",
    "        break\n",
    "        \n",
    "html_code = '<html>' + '<p>Prioridade 1 -->' + html_keys1 + '</p><p>Prioridade 2 -->' + html_keys2 + '</p><p>Prioridade 3 -->' + html_keys3 + '</p>' + html_links + '</html>'\n",
    "\n",
    "f.write(html_code)\n",
    "\n",
    "webbrowser.open(file_name_html)\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
